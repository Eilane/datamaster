# Integra√ß√£o de Dados Clientes PJ

## Sum√°rio

1. [Objetivo](#objetivo)
2. [Caso de Uso](#caso-de-uso)
3. [Arquitetura](#arquitetura-da-solu√ß√£o)
4. [Monitoramento e Observabilidade](#monitoramento-e-observabilidade)  
5. [Fun√ß√µes e Pipelines de Dados](#Pipelines-de-dados)
6. [Resili√™ncia dos Pipelines de Dados](#Resili√™ncia-dos-pipelines-de-dados)
7. [Tabelas](#cadastro-de-tabelas-e-modelagem-de-dados)  
8. [Qualidade de Dados](#motor-de-qualidade-de-dados)  
9. [Expurgo de Dados](#expurgo-de-dados)  
10. [Plano de Recupera√ß√£o e Local de Armazenamento](#plano-de-recupera√ß√£o-e-local-de-armazenamento)  
11. [Visualiza√ß√£o de Dados](#visualiza√ß√£o-de-dados)
12. [Solu√ß√£o T√©cnica](#solu√ß√£o-t√©cnica)
13. [An√°lise das Tecnologias Escolhidas](#an√°lise-de-mercado-e-tecnologias-escolhidas)  
14. [Refer√™ncias](#refer√™ncias)
---

## 1. Objetivo

O projeto tem como objetivo simular e implementar um caso de uso concreto, abordando os principais conceitos e fundamentos essenciais de uma plataforma de dados moderna.  
A solu√ß√£o ser√° focada na disponibiliza√ß√£o de dados com **alta qualidade**, **seguran√ßa**, **monitoramento eficiente** e **escalabilidade**, assegurando uma gest√£o de dados **robusta, confi√°vel e sustent√°vel**.


## 2. Caso de Uso

**Empresa:** CrediF√°cil Brasil *(fict√≠cia)*  
**Projeto:** Integra√ß√£o dados p√∫blicos - Fase 1 Empresas  
**Caso de Uso:** Integra√ß√£o de Dados Clientes PJ  

### **2.0.1 - Descri√ß√£o**
Coletar, analisar e disponibilizar dados cadastrais de clientes PJ e dados p√∫blicos de empresas com uma base da receita federal, com foco em qualidade, governan√ßa e integra√ß√£o ao ecossistema anal√≠tico corporativo.

### **2.0.2 - Escopo do Caso de Uso**

- **Obten√ß√£o de dados fict√≠cios de clientes (PJ):**  
  Simular a ingest√£o de cadastros de clientes pessoa jur√≠dica pertencentes a uma empresa fict√≠cia do setor de empr√©stimos.

- **Integra√ß√£o com dados p√∫blicos de empresas (CNPJ):**  
  Realizar a extra√ß√£o de dados p√∫blicos junto ao Minist√©rio da Fazenda.

- **Armazenamento no Data Lake corporativo:**  
  Persistir todos os dados obtidos no Data Lake.

- **Verifica√ß√£o da situa√ß√£o cadastral da empresa:**  
  Validar se a empresa encontra-se em situa√ß√£o ativa, inativa, inapta ou baixada.

- **An√°lise de poss√≠veis irregularidades:**  
  Identificar poss√≠veis inconsist√™ncias ou comportamentos suspeitos nos dados de CNPJ.


- **An√°lise de potenciais clientes:**  
  Analisar potenciais clientes para promover campanhas de cr√©dito


### **2.0.3 - Diagrama do Caso de Uso**

![alt text](image-1.png)


### 2.0.4 - Riscos Operacionais Mapeados na utiliza√ß√£o de Dados P√∫blicos

### Matriz de Riscos Mapeados

| **Risco** | **Impacto Potencial** | **Estrat√©gia de Mitiga√ß√£o** |
|-----------|------------------------|-----------------------------|
| Altera√ß√£o no layout da p√°gina ou modifica√ß√µes na estrutura dos datasets sem comunica√ß√£o pr√©via | Interrup√ß√£o na extra√ß√£o autom√°tica dos dados, ocasionando falhas no pipeline | Utilizar os dados apenas para fins anal√≠ticos, reduzindo o impacto de interrup√ß√µes tempor√°rias para o cliente final nos sistemas transacionais |
| Presen√ßa de dados duplicados nos arquivos disponibilizados | Aumento no volume de armazenamento e risco de an√°lises incorretas | Aplicar deduplica√ß√£o no processamento e gerar relat√≥rios de controle para acompanhamento cont√≠nuo |


### 2.0.5 Premissas

- Atualiza√ß√£o dos **dados de clientes** do sistema de cr√©dito no Data Lake.  
- Lat√™ncia m√°xima de **15 minutos** para refletir as altera√ß√µes.  
- Objetivo: **evitar o envio de campanhas de cr√©dito** para clientes j√° cadastrados na base de dados.


## 3. Arquitetura

### 3.0.1 Desenho de Arquitetura
Para viabilizar o caso de uso descrito no item 2, os dados ser√£o extra√≠dos diretamente de suas fontes de origem e integrados a uma arquitetura de dados Lakehouse na nuvem p√∫blica Microsoft Azure.

![alt text](image-2.png)


### 3.0.2 - Estrutura l√≥gica das camadas do Data Lake

Os dados est√£o organizados no Data Lake conforme o padr√£o de design da arquitetura Medallion, que estrutura a informa√ß√£o em camadas l√≥gicas (Bronze, Silver e Gold), foi adicionado a camada adicional "raw" que mant√©m os dados em seu formato original (csv,parquet,json). 

As informa√ß√µes de auditoria, monitoramento e qualidade foram separadas dos dados de neg√≥cio e ser√£o armazenadas na camada Governance, garantindo maior organiza√ß√£o e controle sobre os metadados e processos de governan√ßa.

<img width="575" height="493" alt="image" src="https://github.com/user-attachments/assets/5741328e-1c23-478b-84f2-7f8f5b0fb715" />



### 3.0.3 ‚Äì Integra√ß√£o dos Recursos com o Data Lake Gen2

Recursos com acesso direto ao Data Lake Corporativo
<img width="969" height="612" alt="image" src="https://github.com/user-attachments/assets/314c2638-0b1f-4482-81fe-07b24b0351fc" />





### 3.0.4 ‚Äì Estrutura do Terraform

Este reposit√≥rio organiza a infraestrutura como c√≥digo (IaC) utilizando **Terraform**, com m√≥dulos separados por servi√ßo para facilitar manuten√ß√£o e reutiliza√ß√£o.

```bash
terraform
‚îú‚îÄ‚îÄ main.tf              # Arquivo principal com recursos globais e providers
‚îú‚îÄ‚îÄ variables.tf         # Declara√ß√£o de vari√°veis
‚îú‚îÄ‚îÄ terraform.tfvars     # Valores sens√≠veis | N√£o est√° no git
‚îú‚îÄ‚îÄ provider.tf          # Configura√ß√£o do provider
‚îú‚îÄ‚îÄ modules
‚îÇ   ‚îú‚îÄ‚îÄ resource_group
‚îÇ   ‚îÇ   # M√≥dulo cria o Resource Group principal do projeto
‚îÇ   ‚îÇ  
‚îÇ   ‚îú‚îÄ‚îÄ azure_sql        # M√≥dulo cria o SQL DATABASE
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scripts      # Scripts SQL, como init_credito.sql para tabelas e CDC
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ databricks
‚îÇ   ‚îÇ   # M√≥dulo cria os recursos abaixo:
‚îÇ   ‚îÇ   # - workspace
‚îÇ   ‚îÇ   # - managed_identity
‚îÇ   ‚îÇ   # - metastore
‚îÇ   ‚îÇ   # - storage_credential
‚îÇ   ‚îÇ   # - external_locations
‚îÇ   ‚îÇ   # - catalog
‚îÇ   ‚îÇ   # - schemas
‚îÇ   ‚îÇ   # - notebooks (bronze, silver, gold, governance)
‚îÇ   ‚îÇ   # - cluster
‚îÇ   ‚îÇ   # - jobs
‚îÇ   ‚îÇ   # - roles
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ data_factory
‚îÇ   ‚îÇ   # M√≥dulo cria:
‚îÇ   ‚îÇ   # - linked_services
‚îÇ   ‚îÇ   # - datasets
‚îÇ   ‚îÇ   # - pipelines
‚îÇ   ‚îÇ   # - triggers
‚îÇ   ‚îÇ   # - roles
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ functions_app
‚îÇ   ‚îÇ   # M√≥dulo cria fun√ß√£o Python
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ keyvault
‚îÇ   ‚îÇ   # M√≥dulo cria Key Vault e secrets
‚îÇ   ‚îÇ   # - senha_db
‚îÇ   ‚îÇ   # - role 
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ storage_account
‚îÇ       # M√≥dulo cria os containers:
‚îÇ       # - raw
‚îÇ       # - bronze
‚îÇ       # - silver
‚îÇ       # - gold
‚îÇ       # - governance
‚îÇ       # - Politicas 

```

### 3.0.4 ‚Äì Premissas e Pol√≠ticas

- O **Data Factory** ser√° a principal fonte de comunica√ß√£o com dados externos, garantindo integra√ß√£o controlada e monitorada.

- O acesso aos recursos no **Data Lake** ser√° restrito ao **Unity Catalog** e ao **Data Factory**, utilizando **Managed Identity** para autentica√ß√£o segura.

- As aplica√ß√µes, **com exce√ß√£o dos recursos mencionados acima**, n√£o poder√£o acessar diretamente o Data Lake corporativo, garantindo isolamento e seguran√ßa dos dados.

- Para **redu√ß√£o de custos**, os dados da camada **raw** ter√£o um **lifecycle de 6 meses**; ap√≥s esse per√≠odo, ser√£o expurgados automaticamente.

- Os dados das demais camadas ser√£o movidos para uma **camada cold storage** ap√≥s **5 anos**, permitindo reten√ß√£o de longo prazo com custo reduzido.

## 4. Monitoramento, Auditoria e Controle de Ativos
![alt text](image-5.png)

## 5. Fun√ß√µes e Pipelines de Dados

### **5.0.1 - Azure Function** ‚Äì `funcaoreceita`

**Descri√ß√£o:** Fun√ß√£o respons√°vel por extrair todas as URLs v√°lidas dos arquivos dispon√≠veis para download, a partir do HTML do site:  
`https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/{year_month}/`,  
**Entrada:**  
- `year_month` (formato `YYYY-MM`)  
**Sa√≠da:**  
- JSON contendo uma lista de URLs v√°lidas para download dos arquivos.


### **5.0.2 - Pipeline de Dados** ‚Äì `pipeline_ingest_dados_pj`
**Descri√ß√£o:** Pipeline respons√°vel por capturar e armazenar dados p√∫blicos de empresas no Data Lake.  
**Periodicidade:** Mensal  
**Dia:** √öltimo dia do m√™s  
**Hor√°rio:** 09:00  

![alt text](image-4.png)

### 5.0.3 - Pipeline: `pipeline_ingest_lake`
**Descri√ß√£o:** Pipeline respons√°vel por atualizar as tabelas bronze, silvers, gold no Data Lake.  
<img width="1034" height="316" alt="image" src="https://github.com/user-attachments/assets/8f2496dc-8948-4f37-b313-190adc6bfa70" />

## 6. Resili√™ncia dos Pipelines de Dados

### 6.0.1 - Pipeline: `pipeline_ingest_dados_pj`

**Reprocessamento:**  
O pipeline permite informar o ano e m√™s a serem reprocessados no formato `YYYY-MM`.  
Caso nenhum valor seja fornecido, o pipeline utiliza automaticamente o ano e m√™s atuais.

**Configura√ß√£o de retomada (Retry):**  
- At√© 3 tentativas em caso de falha  
- Intervalo de 60 segundos entre cada tentativa


## 7. Tabelas

###  7.0.1 - SCD - Slowly Changing Dimensions  

T√©cnica de gerenciamento de dados que define como as tabelas lidam com informa√ß√µes que mudam ao longo do tempo.  

#### Aplicado Tipo 0 nas Tabelas *Bronze* 
  - Sem altera√ß√µes, apenas append
#### Aplicado Tipo 1 nas Tabelas *Silver* e *Gold*  

- **Tipo 1**: Sem hist√≥rico ‚Äî apenas os dados atuais s√£o mantidos.  
  - **Forma de escrita**: *Overwrite*  

###  7.0.2 - Organiza√ß√£o das tabelas

<img width="886" height="613" alt="image" src="https://github.com/user-attachments/assets/dacadf5e-77ad-400e-b57a-34d4c424254e" />

## 8. Qualidade de Dados
### 8.0.1 - Tabela com regras de qualidade

**Objetivo:**  
Registrar as **regras de qualidade** que devem ser aplicadas nas tabelas **Silver** e **Gold** para garantir a integridade e consist√™ncia dos dados.

**Tabela:** `prd.governance.regras_qualidade`

**Descri√ß√£o:**  
- Centraliza todas as regras de valida√ß√£o de dados.  
- Permite auditoria e monitoramento da conformidade dos dados.  

**Exemplo de uso:**  
- Validar campos obrigat√≥rios (`NOT NULL`)  
- Calcular scores de qualidade por tabela e coluna

<img width="988" height="430" alt="image" src="https://github.com/user-attachments/assets/5c066b05-2a8c-45d8-bce8-6d90a0f697a6" />


## 9. Expurgo de Dados
- Lifecycle de 6 meses na camada raw -> Configura√ß√£o realizada no storage account
- Rotina semanal de Vaccum para remo√ß√£o de arquivos obsoletos 
  - Periodicidade: Semanal (S√°bado)
  - Hor√°rio: 06:00

## 10. Visualiza√ß√£o de Dados
*Em desenvolvimento...*

## 11 Acompanhamento de custos por recurso via portal
![alt text](image-3.png)

## 12. Solu√ß√£o T√©cnica

### 12.0.1 Pr√©-requisitos

- **Assinatura do Azure** com permiss√µes administrativas 
- **Conta no Azure Databricks Enterprise** [Identificar o Account ID](https://accounts.azuredatabricks.net) 
- **Azure CLI**: [Instalar](https://aka.ms/installazurecliwindows) 
- **Terraform**: [Download](https://www.terraform.io/downloads.html)
- **SQLCMD**:[Download](https://learn.microsoft.com/en-us/sql/tools/sqlcmd/sqlcmd-download-install?view=sql-server-ver17&tabs=windows)
- **Visual Studio Code**: [Download](https://code.visualstudio.com/download)  
  - Extens√µes recomendadas:  
    - **Azure Resources**  
    - **Microsoft Terraform**  
    - **HashiCorp Terraform**  
- **Meu Ip** [Link](https://meuip.com.br/) Liberar o firewall no banco de dados 
---

### 12.0.2 Passo a passo


1. **Criar o arquivo de vari√°veis**:

Dentro da pasta `terraform`, crie o arquivo `terraform.tfvars`:

**Nome do arquivo:** `terraform.tfvars`

**Conte√∫do:**
```bash
senha_db        = ""  # Exemplo de senha v√°lida:!CfacilBr489@demo  A senha deve conter letras mai√∫sculasmin√∫sculas, n√∫meros e caracteres especiais
subscription_id = ""  # ID da Subscription
account_id      = ""  # ID da Account
meu_ip          = ""  # IP a ser Liberado no Firewall 
```
 

2. **Executar os comandos abaixo no terminal do visual code**:

```bash
cd terraform         # Entrar na pasta terraform
az login             # Autentica na conta Azure
terraform init       # Inicializa os providers e m√≥dulos
terraform plan       # Mostra o que ser√° criado
terraform apply      # Aplica as mudan√ßas na Azure
```

3. **Aceite a cria√ß√£o dos recursos digitando "yes"**

<img width="480" height="110" alt="image" src="https://github.com/user-attachments/assets/f5903a29-34ef-4000-8938-a2ded23457c6" />

4. **Deploy do c√≥digo da Fun√ß√£o na Azure**:

Click com o bot√£o direito do mouse "Deploy to Function App" para subir o c√≥digo da fun√ß√£o python para Azure.
<img width="471" height="511" alt="image" src="https://github.com/user-attachments/assets/c3a3cc23-0168-4b5a-bdca-07285ee03b80" />


5. **Execute o comando abaixo no terminal para liberar acesso a Manage Identity do Workspace para ler os logs de execu√ß√£o dos pipelines no Data Factory, substitua apenas o id da subscription**
```bash
az role assignment create `
  --assignee $(az identity show --name "dbmanagedidentity" --resource-group "databricks-rg-rgprdcfacilbr" --query principalId -o tsv) `
  --role "Reader" `
  --scope "/subscriptions/<subscription-id>/resourceGroups/rgprdcfacilbr/providers/Microsoft.DataFactory/factories/adfcfacilbr"
```

6. **Executar oo comando abaixo para criar a tabela no Banco de Dados. Obs: O script init_credito.sql est√° na pasta terraform/modules/azure_sql**:
```bash
sqlcmd -S tcp:sqlcfacilbr.database.windows.net -d sqlcfacilbr -U sqladmin -P "InformarSenhaBanco" -i init_credito.sql
```


6. **DESTRUIR O AMBIENTE**:
   N√£o esquecer, para n√£o gerar cobran√ßa adicinal üò≠ 
```bash
terraform destroy       # Extrui todos os recursos
```

Pode ser que voc√™ leve um erro para destruir os schemas. Mas, calma, nesse caso, voce precisa entrar no portal e excluir manualmente o grupo de recursos.
 ```bash
 Error: cannot delete schema: Schema 'prd.b_cfacil_credito' is not empty. The schema has 1 tables(s), 0 functions(s), 0 volumes(s)
 Error: cannot delete schema: Schema 'prd.s_rf_empresas' is not empty. The schema has 4 tables(s), 0 functions(s), 0 volumes(s)
```

## 13. An√°lise das Tecnologias Escolhidas

### 13.0.1 Microsoft - Plataforma de Integra√ß√£o como Servi√ßo 

A Microsoft foi nomeada l√≠der no Quadrante M√°gico‚Ñ¢ da Gartner¬Æ de 2024 para Plataforma de Integra√ß√£o como Servi√ßo 
O Azure Integration Services ‚Äî Inclui o Azure API Management, o Azure Logic Apps, o Azure Service Bus, o Azure Event Grid, o Azure Functions e o **Azure Data Factory**

<img width="656" height="739" alt="image" src="https://github.com/user-attachments/assets/bfad38f6-5e0c-4823-89f2-8a52aca3e82c" />


### 13.0.2 Databricks 

Databricks como L√≠der no Quadrante M√°gico‚Ñ¢ da Gartner¬Æ de 2025 para Plataformas de Ci√™ncia de Dados e Aprendizado de M√°quina.
<img width="730" height="748" alt="image" src="https://github.com/user-attachments/assets/3b941f6b-ce65-412c-aebb-3e526e39595a" />



## 14. Refer√™ncias

databricks_grant Resource
https://registry.terraform.io/providers/databricks/databricks/latest/docs/resources/grant


Captura de dados alterados (CDC) com o Banco de Dados SQL do Azure
https://learn.microsoft.com/en-us/azure/azure-sql/database/change-data-capture-overview?view=azuresql


Microsoft.DataFactory factories/adfcdcs
https://learn.microsoft.com/en-us/azure/templates/microsoft.datafactory/factories/adfcdcs?pivots=deployment-language-bicep


SQLCMD
https://learn.microsoft.com/en-us/sql/tools/sqlcmd/sqlcmd-download-install?view=sql-server-ver17&tabs=windows

Use Azure managed identities in Unity Catalog to access storage
https://learn.microsoft.com/en-us/azure/databricks/connect/unity-catalog/cloud-storage/azure-managed-identities

TERRAFORM -> 
https://www.youtube.com/watch?v=YlY9WMURc50&t=661s    
https://registry.terraform.io/providers/hashicorp/azurerm/4.39.0/docs  
https://www.youtube.com/watch?v=YwloG_qq5aA 
https://registry.terraform.io/modules/Azure/naming/azurerm/latest#output_data_factory

UNITY CATALOG
https://learn.microsoft.com/pt-br/azure/databricks/data-governance/unity-catalog/

OUTROS
https://www.gartner.com/reviews/market/cloud-database-management-systems/compare/product/amazon-simple-storage-service-amazon-s3-vs-azure-data-lake
https://www.databricks.com/blog/databricks-named-leader-2025-gartner-magic-quadrant-data-science-and-machine-learning
https://azure.microsoft.com/en-us/blog/microsoft-named-a-leader-in-2024-gartner-magic-quadrant-for-integration-platform-as-a-service/

DEBEZIUM
https://debezium.io/documentation/reference/stable/connectors/postgresql.html (Debezium connector for PostgreSQL)

https://github.com/ycamargo/debezium-on-aks
